{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## How well do volunteers do against the aggregate model?\n",
        "\n",
        "Note this requires the `volunteer_skill_exploration.ipynb` notebook to have been on each `subject_id` and the output notebooks to be saved to `/volunteer_losses`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from gzbuilder_analysis.parsing import unmake_json\n",
        "from gzbuilder_analysis.fitting import Model, loss\n",
        "import lib.galaxy_utilities as gu\n",
        "import scrapbook as sb"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_models = pd.read_pickle('lib/fitted_models.pickle')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_losses = fitted_models.agg_loss"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nbs = sb.read_notebooks('volunteer_losses')\n",
        "loss_df = pd.concat([\n",
        "  pd.Series(d.get('losses', np.nan)).rename(d.get('subject_id', np.nan))\n",
        "  for d in (j.scraps.data_dict for i, j in nbs.items())\n",
        "], axis=1).T"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_better_than_agg = pd.Series([])\n",
        "for name in loss_df.columns:\n",
        "  res = loss_df[name].dropna()\n",
        "  if len(res) > 0:\n",
        "    better = (res - agg_losses.reindex(res.index) < 0)\n",
        "    p_better_than_agg.loc[name] = (better < 0).astype(int).sum() / len(res)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\mathrm{Ability} = \\beta(1 + N_\\mathrm{better}, 1 + N_\\mathrm{classifications} - N_\\mathrm{better})$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 1, 500)\n",
        "five_percentile = pd.Series([])\n",
        "ninety_five_percentile = pd.Series([])\n",
        "means = pd.Series([])\n",
        "medians = pd.Series([])\n",
        "plt.figure(figsize=(16, 4), dpi=100)\n",
        "for name in loss_df.columns:\n",
        "  res = loss_df[name].dropna()\n",
        "  if len(res) >= 10:\n",
        "    better = (res - agg_losses.reindex(res.index) < 0)\n",
        "    dist = st.beta.pdf(x, 1 + better.sum(), 1 + len(res) - better.sum())\n",
        "    rvs = st.beta.rvs(1 + better.sum(), 1 + len(res) - better.sum(), size=10000)\n",
        "    dist_median = st.beta.median(1 + better.sum(), 1 + len(res) - better.sum())\n",
        "    medians.loc[name] = st.beta.median(1 + better.sum(), 1 + len(res) - better.sum())\n",
        "    means.loc[name] = st.beta.mean(1 + better.sum(), 1 + len(res) - better.sum())\n",
        "    five_percentile.loc[name] = np.percentile(rvs, 5)\n",
        "    ninety_five_percentile.loc[name] = np.percentile(rvs, 95)\n",
        "    plt.fill_between(x, 0, dist, alpha=0.05, color='k')\n",
        "plt.xlim(0, 1);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 4), dpi=100)\n",
        "for i, name in enumerate(('klmasters', 'tingard', 'ElisabethB')):\n",
        "  res = loss_df[name].dropna()\n",
        "  c = 'C{}'.format(i)\n",
        "  better = (res - agg_losses.reindex(res.index) < 0)\n",
        "  dist = st.beta.pdf(x, 1 + better.sum(), 1 + len(res) - better.sum())\n",
        "  plt.fill_between(x, 0, dist, alpha=0.2, color=c)\n",
        "  plt.plot(x, dist, c, linewidth=0.5, alpha=1, label='{} ({} classifications)'.format(name, len(res)))\n",
        "plt.legend()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.kdeplot(ninety_five_percentile, shade=True, label='95% upper bound')\n",
        "sns.kdeplot(medians, shade=True, label='medians')\n",
        "sns.kdeplot(means, shade=True, label='means')\n",
        "ninety_five_percentile.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what does this all mean? It means that the best individual model consistently outperforms the aggregate (around 70% of the time). However, for any individual volunteer, we can say with greater than 95% confidence that their model will be worse than the tuned aggregate model more than half the time. "
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}